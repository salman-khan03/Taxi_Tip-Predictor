# ğŸš• Taxi Tip Predictor - Real ML Project with XGBoost & GPU Acceleration

A professional-grade machine learning project that predicts taxi tips using XGBoost with NVIDIA GPU acceleration. This project demonstrates real-world data science workflows including data cleaning, memory optimization, and GPU-accelerated machine learning.

## ğŸ’¡ What You'll Learn

- **Handling Real-World Datasets**: Cleanup, missing values, anomalies, aggregation
- **Solving Memory Limitations**: Using cuDF (GPU-accelerated Pandas) + RMM for memory management
- **GPU-Accelerated ML**: XGBoost on NVIDIA GPUs for faster training
- **Model Evaluation**: Comprehensive performance metrics and validation
- **Professional Workflow**: Think like a data scientist, solve problems systematically

## ğŸš€ Environment Setup

### Option 1: Google Colab (Recommended for Beginners)

1. Open Google Colab
2. Change runtime to **T4 GPU** (Runtime â†’ Change runtime type â†’ GPU: T4)
3. Use the smaller dataset (5 million rows)
4. Install dependencies (see `setup_colab.ipynb`)

### Option 2: Local Setup (Full Dataset - 38M Rows)

**Requirements:**
- CUDA-compatible GPU (NVIDIA)
- WSL (Windows Subsystem for Linux) - **MUST**
- Miniforge/Conda - **MUST**
- Follow current RAPIDS installation guide for your CUDA version

**Installation Steps:**

1. **Install RAPIDS** (check [RAPIDS Installation Guide](https://rapids.ai/start.html)):
   ```bash
   conda create -n rapids-env -c rapidsai -c conda-forge -c nvidia \
       cudf=24.04 python=3.10 cudatoolkit=11.8
   conda activate rapids-env
   ```

2. **Install XGBoost with GPU support**:
   ```bash
   pip install xgboost
   ```

3. **Install other dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“Š Dataset

Download the NYC Taxi dataset:
- **Colab version**: 5 million rows (smaller subset)
- **Local version**: 38 million rows (full dataset)

Place the dataset in the `data/` directory.

## ğŸ—ï¸ Project Structure

```
taxi-tip-predictor/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup_colab.ipynb          # Colab setup notebook
â”œâ”€â”€ config.py                   # Configuration settings
â”œâ”€â”€ data_loader.py              # Data loading and preprocessing
â”œâ”€â”€ feature_engineering.py      # Feature engineering pipeline
â”œâ”€â”€ model_trainer.py            # XGBoost model training
â”œâ”€â”€ model_evaluator.py          # Model evaluation and metrics
â”œâ”€â”€ hyperparameter_tuning.py    # Hyperparameter tuning module
â”œâ”€â”€ main.py                     # Main training script (K-Fold validation)
â”œâ”€â”€ main_advanced.py            # Advanced training (with hyperparameter tuning)
â””â”€â”€ data/                       # Dataset directory
    â””â”€â”€ (place your CSV files here)
```

## âš¡ Quick Start

1. **Check your environment**:
   ```bash
   python check_environment.py
   ```

2. **Install dependencies** (if not already installed):
   ```bash
   pip install -r requirements.txt
   ```

3. **Download the NYC Taxi dataset** (`Distilled_2023_Yellow_Taxi_Trip_Data.csv`) and place it in the `data/` directory

4. **Update config.py** if needed (dataset filename, paths, etc.)

5. **Run training**:
   - **Basic workflow** (K-Fold validation):
     ```bash
     python main.py
     ```
   - **Advanced workflow** (with hyperparameter tuning):
     ```bash
     python main_advanced.py
     ```

## ğŸ¯ Usage

### Training the Model

```bash
python main.py
```

### Configuration

Edit `config.py` to adjust:
- Dataset path
- GPU memory settings
- Model hyperparameters
- Training parameters

## ğŸ“ˆ Features

- **GPU-Accelerated Data Processing**: cuDF for fast data manipulation
- **Memory Management**: RMM for efficient GPU memory allocation
- **Feature Engineering**: Time-based, distance, and aggregated features
- **Model Training**: XGBoost with GPU acceleration
- **Comprehensive Evaluation**: RMSE, MAE, RÂ², and feature importance

## ğŸ§  What Makes This Different

This isn't a beginner demoâ€”it's a **real workflow** based on:
- Real data challenges (huge datasets, missing values, anomalies)
- Real problems (CPU/GPU memory limits, runtime crashes)
- Professional solutions (explained step-by-step)
- Decision-making rationale (why we make each choice)

## ğŸ“ License

This project is for educational purposes.

## ğŸ™ Acknowledgments

Based on professional data science workflows and best practices for GPU-accelerated machine learning.
